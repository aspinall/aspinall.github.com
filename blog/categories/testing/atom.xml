<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: testing | Craig Aspinall]]></title>
  <link href="http://www.craigaspinall.com/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://www.craigaspinall.com/"/>
  <updated>2012-06-28T16:29:33+10:00</updated>
  <id>http://www.craigaspinall.com/</id>
  <author>
    <name><![CDATA[Craig Aspinall]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Something to prove]]></title>
    <link href="http://www.craigaspinall.com/blog/2012/06/26/something-to-prove/"/>
    <updated>2012-06-26T06:57:00+10:00</updated>
    <id>http://www.craigaspinall.com/blog/2012/06/26/something-to-prove</id>
    <content type="html"><![CDATA[<p>I am building a simple web application and have set myself the challenge of being able to continuously deploy it to the cloud. To do that, I need to be confident that any changes I make don't break existing functionality. However, the bulk of the application will run in the client using a combination of HTML, CSS and Javascript, and testing those technologies is much more difficult than your typical Java (or JVM based) application. After banging my head against this on and off for several weeks, I was forced to stop and ask myself the question "what is it  that I am actually trying to prove"?</p>

<!-- more -->


<p>I guess at an abstract level I'm trying to prove that there are no unintended side effects for any changes that I make to the application. The application in question is a big visible clock that can be used when facilitating meetings, workshops or events. It's simple in that there is very little logic and no persistence layer, but difficult to test because most of the logic resides on the client, in what most web frameworks would consider to be the view layer.</p>

<p>Whilst <a href="http://aspiringcraftsman.com/2007/08/25/interactive-application-architecture/">researching</a> how to achieve this, I came across the <a href="http://atomicobject.com/files/PresenterFirstAgile2006.pdf">Presenter First</a> process and corresponding derivative of the <a href="http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93presenter">Model-View-Presenter</a> pattern (which is actually the <a href="http://martinfowler.com/eaaDev/PassiveScreen.html">Passive View</a> pattern I believe). The goal of Presenter First is to make interactive applications suitable for test driven development, which it does by making the "view" as thin (read dumb or simple) as possible, and pushing all the presentation logic in to the <em>presenter</em>, thereby completely isolating the view from the underlying domain model (unlike the traditional Model-View-Controller or Model-View-Presenter pattern). By defining very clear interfaces between the <em>presenter</em> and the <em>model</em>, and between the <em>presenter</em> and the <em>view</em>, testability increases tremendously, which is good news for me!</p>

<p>It means that both the model and the view can be mocked, and the interactive behaviour of the application can be tested in isolation. Similarly the business logic can be tested in isolation. The only questions are whether or not the view should be tested and if so, is it possible to automate that testing?</p>

<p>It is certainly true that it can be very difficult to build robust and maintainable automated tests for views. They tend to change more frequently than, and independently of, the underlying application behaviour. The real question is whether or not there is enough value in doing so to get a return on the investment you make in building and maintaining the tests?</p>

<p>In this case I could take the risk and not test the view automatically because both the risk and impact of defects in the view are low. I could run a manual <em>sanity check</em> after each commit and confirm that everything still looks OK in production, safe in the knowledge that I can easily roll back the changes in the event that there are issues.</p>

<p>But mine is a small application that is not mission critical to anyone, what about something that is mission critical? I feel like I owe it to the teams that I coach to do this anyway and get an idea of the potential return on investment (or lack thereof) for automating tests for the view. Therefore, in reality, that is what I am trying to prove and where the value will come from, not the testing itself!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Play!ing with Concordion]]></title>
    <link href="http://www.craigaspinall.com/blog/2012/06/14/playing-with-concordion/"/>
    <updated>2012-06-14T07:42:00+10:00</updated>
    <id>http://www.craigaspinall.com/blog/2012/06/14/playing-with-concordion</id>
    <content type="html"><![CDATA[<p>I've just started experimenting with <a href="http://www.playframework.org">Play! framework 2.0</a>. I was attracted by the static typing and functional nature of <a href="http://www.scala-lang.org">Scala</a>, and the shear testability of Play! applications. It looks like you can test each component of the MVC pattern independently and without going through the UI, and I think in most cases without even starting the server.</p>

<p>As someone who is preaching about testability on a daily basis, I have to investigate further. Of course, the proof of the pudding is in the eating, so I'm going to build and deploy a simple application using it, to see whether it lives up to the promise. I'll track the progress of that application and share any learnings here.</p>

<!--more-->


<p>The first thing I wanted to do was set up a framework for <a href="http://www.specificationbyexample.com">specifying by example</a> with my current BDD tool of choice which is <a href="http://www.concordion.org">Concordion</a>. Turns out that isn't as straight forward as I thought, since Play! uses <a href="https://github.com/harrah/xsbt/wiki">SBT</a> (Scala Build Tool), which is new to me, and Concordion has a few quirks in the way it has to be set up. So here's the first of my learnings, which explains how to get Concordion working in a Play! project.</p>

<p>First of all you need to add a dependency on Concordion:</p>

<p>``` scala Build.scala
object ApplicationBuild extends Build {
...
  val appDependencies = Seq(</p>

<pre><code>"org.concordion" % "concordion" % "1.4.2" % "test"
</code></pre>

<p>  )
...
}
```</p>

<p>Next, you need to instruct SBT to copy the Concordion HTML files to the target folder:</p>

<p>``` scala Build.scala
object ApplicationBuild extends Build {
...
   val main = PlayProject(appName, appVersion, appDependencies, mainLang = SCALA).settings(</p>

<pre><code> unmanagedClasspath in Test &lt;+= (baseDirectory) map { bd =&gt; Attributed.blank(bd / "test") }
</code></pre>

<p>   )
...
}
```</p>

<p>And last but not least, you probably want to tell Concordion to store it's reports somewhere sensible (otherwise it puts them in the <code>java.io.tmpdir</code> folder):</p>

<p><code>scala Build.scala
object ApplicationBuild extends Build {
...
  System.setProperty("concordion.output.dir", "target/test-reports/concordion")
...
}
</code></p>

<p>With that in place, you can create a Concordion HTML specification and store it in the <code>test</code> folder of your project:</p>

<p>``` html ConcordionExample.html
&lt;!DOCTYPE HTML>
<html xmlns:concordion="http://www.concordion.org/2007/concordion">
<head>
  <title>Concordion Example</title>
</head>
<body></p>

<pre><code>&lt;h1&gt;Concordion Example&lt;/h1&gt;
&lt;p&gt;The answer always equals &lt;span concordion:assertEquals="value()"&gt;1&lt;/span&gt;&lt;/p&gt;
</code></pre>

<p></body>
</html>
```</p>

<p>Finally, create a Scala or Java fixture class (note that you need to have a dummy method annotated with the <a href="http://www.junit.org">JUnit</a> <code>@Test</code> annotation for the test to be picked up, I haven't found a better way around this yet):</p>

<p>``` scala ConcordionExampleTest.scala
import org.concordion.integration.junit4.ConcordionRunner
import org.junit.runner.RunWith
import org.junit.Test</p>

<p>@RunWith(classOf[ConcordionRunner])
class ConcordionExampleTest {
  def value = 1</p>

<p>  @Test
  def runThisTest() {}
}
```</p>

<p>``` java ConcordionExampleTest.java
import org.concordion.integration.junit4.ConcordionRunner;
import org.junit.runner.RunWith;
import org.junit.Test;</p>

<p>@RunWith(ConcordionRunner.class)
public class ConcordionExampleTest {</p>

<pre><code>public int value() {
        return 1;
}

@Test
public void runThisTest() {}
</code></pre>

<p>}
```</p>

<p>Then you can run <code>play test</code> and voila, one Concordion specification is executed!</p>

<p>``` plain $ play test
$ play test
[info] Loading project definition from /Users/craigaspinall/Work/scala-concordion-poc/project
[info] Set current project to scala-concordion-poc (in build file:/Users/craigaspinall/Work/scala-concordion-poc/)
[info] Compiling 1 Scala source to /Users/craigaspinall/Work/scala-concordion-poc/target/scala-2.9.1/test-classes...
[info] ConcordionExampleTest
/Users/craigaspinall/Work/scala-concordion-poc/target/test-reports/concordion/ConcordionExample.html
Successes: 1, Failures: 0</p>

<p>[info] + ConcordionExampleTest.runThisTest
[info] + ConcordionExampleTest.[Concordion Specification for 'ConcordionExample']
[info]
[info]
[info] Total for test ConcordionExampleTest
[info] Finished in 0.526 seconds
[info] 2 tests, 0 failures, 0 errors
[info] Passed: : Total 2, Failed 0, Errors 0, Passed 2, Skipped 0
[success] Total time: 2 s, completed 14/06/2012 8:25:08 AM
```</p>

<p>If anyone reading this knows how to avoid the dummy test, then please leave a comment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The role of automated testing]]></title>
    <link href="http://www.craigaspinall.com/blog/2010/11/28/the-role-of-automated-testing/"/>
    <updated>2010-11-28T00:00:00+10:00</updated>
    <id>http://www.craigaspinall.com/blog/2010/11/28/the-role-of-automated-testing</id>
    <content type="html"><![CDATA[<p>Over the last week I've had a lot of discussions about the underlying motive of the roles that my colleagues and I are currently performing. We are responsible for improving the quality of software being produced by the teams we are involved with, and the main focus of our recent work has been creating automation tools to assist the testers. There are three possible motives for our roles:</p>

<h1>to reduce costs</h1>

<h1>to increase productivity</h1>

<h1>to improve quality</h1>

<p>For us it's a simple case of improving quality but that doesn't necessarily mean that our leaders have the same motives. In fairness they do overlap, but focusing too much on any one of them can be to the detriment of the others.</p>

<!--more-->


<p>For example, if you want to reduce costs and automated testing makes the testers four times more productive, then in theory you can remove three quarters of your testers without any loss of quality or productivity. Of course the theory and reality are quite different! Similarly, if you go all out for quality, you may need more people or tools or equipment which will result in increased costs, and progress may be slower.</p>

<p>The catalyst for the discussions was that we just found out that the number of testers is being reduced on one of the projects we're working on and we were horrified. We were very concerned that the work we have been doing was being used to justify cost reductions but fortunately that isn't the case (at least not this time).</p>

<p>Since our goal is to improve quality we expect that any cost savings we enable during development are re-investing into improving quality in other ways. This should include freeing up the testers to do more exploratory testing, but could also include things like performance, security or usability testing that often get overlooked.</p>

<p>Even with that investment, we expect that there will still be an overall reduction in cost, simply because less defects will get to production. The cost of fixing defects increases exponentially the later in the process they are found, because more and more people become involved in the development, testing and technical support of the fix. Unfortunately the total cost of ownership is not easy to quantify and rarely gets the focus it deserves.</p>

<p>"Episode 18":http://goo.gl/Z69dl of the "Coding By Numbers":http://www.codingbynumbers.com podcast (which I co-host) has a much more in depth discussion on this subject if you want to hear more!</p>
]]></content>
  </entry>
  
</feed>
